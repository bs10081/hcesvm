================================================================================
Hierarchical CE-SVM - Inverted Strategy
Dataset: TAE
Timestamp: 2026-01-30 21:05:51
================================================================================

Loading training data from: /home/bs10081/Developer/NSVORA/Archive/tae_split.xlsx

Loaded multi-class data:
  Class 1: 39 samples
  Class 2: 40 samples
  Class 3: 41 samples
  Features: 6
Training data loaded:
  Class 1: 39 samples
  Class 2: 40 samples
  Class 3: 41 samples
  Features: 6

================================================================================
Model Parameters
================================================================================
  C_hyper: 1.0
  epsilon: 0.0001
  M: 1000.0
  time_limit: 1800
  mip_gap: 0.0001
  threads: 0
  verbose: True
  enable_selection: True
  feat_upper_bound: 1000
  feat_lower_bound: 1e-07

================================================================================
Training Hierarchical Classifier - Inverted Strategy
================================================================================

Inverted Strategy:
  H1: medium class vs {majority, minority}
  H2: {medium, majority} vs minority
  (Class roles determined dynamically based on sample counts)

============================================================
Training Hierarchical CE-SVM (Strategy: inverted)
============================================================
Class 1: 39 samples
Class 2: 40 samples
Class 3: 41 samples
Features: 6

Dynamic Class Roles:
  Majority:  Class 3 (41 samples)
  Medium:    Class 2 (40 samples)
  Minority:  Class 1 (39 samples)

============================================================
Training H1: Class 2 (+1) vs {Class 3, 1} (-1)
============================================================
H1 Training samples: 120
  Positive (+1): 40 samples
  Negative (-1): 80 samples

Set parameter TimeLimit to value 1800
Set parameter MIPGap to value 0.0001
Set parameter OutputFlag to value 1
Set parameter Threads to value 0
Gurobi Optimizer version 13.0.1 build v13.0.1rc0 (linux64 - "Ubuntu 24.04.3 LTS")

CPU model: AMD Ryzen 9 9900X 12-Core Processor, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 20 physical cores, 20 logical processors, using up to 20 threads

Non-default parameters:
TimeLimit  1800

Optimize a model with 854 rows, 501 columns and 3278 nonzeros (Min)
Model fingerprint: 0xc8d15649
Model has 374 linear objective coefficients
Variable types: 135 continuous, 366 integer (366 binary)
Coefficient statistics:
  Matrix range     [1e-07, 1e+03]
  Objective range  [1e+00, 1e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 2e+02]

Presolve removed 204 rows and 120 columns
Presolve time: 0.00s
Presolved: 650 rows, 381 columns, 2442 nonzeros
Variable types: 13 continuous, 368 integer (276 binary)
Found heuristic solution: objective 360.0000000
Found heuristic solution: objective 358.0000000
Found heuristic solution: objective 118.0000000

Root relaxation: objective -1.798000e+00, 279 iterations, 0.00 seconds (0.01 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0   -1.79800    0   93  118.00000   -1.79800   102%     -    0s
H    0     0                      79.0000000   -1.79800   102%     -    0s
     0     0    0.39880    0  217   79.00000    0.39880  99.5%     -    0s
     0     0    1.17667    0  225   79.00000    1.17667  98.5%     -    0s
     0     0    2.24492    0  184   79.00000    2.24492  97.2%     -    0s
     0     0    2.24492    0  195   79.00000    2.24492  97.2%     -    0s
     0     0    2.24492    0  195   79.00000    2.24492  97.2%     -    0s
     0     0    4.34526    0  242   79.00000    4.34526  94.5%     -    0s
     0     0    4.46346    0  227   79.00000    4.46346  94.4%     -    0s
     0     0    4.60009    0  230   79.00000    4.60009  94.2%     -    0s
     0     0    4.64683    0  236   79.00000    4.64683  94.1%     -    0s
     0     0    6.43793    0  223   79.00000    6.43793  91.9%     -    0s
     0     0    6.44648    0  223   79.00000    6.44648  91.8%     -    0s
     0     0    6.44648    0  223   79.00000    6.44648  91.8%     -    0s
     0     2    6.44648    0  223   79.00000    6.44648  91.8%     -    0s
 51089 22409   60.61954   43  267   79.00000   46.69915  40.9%  35.0    5s
 69740 29386   59.99287   39  249   79.00000   48.71831  38.3%  37.4   10s
 73657 31016   55.98650   41  351   79.00000   51.05390  35.4%  41.4   15s
 89210 35809   60.40831   46  342   79.00000   55.69756  29.5%  50.9   20s
 103365 39597   60.03962   50  268   79.00000   57.34148  27.4%  58.8   25s
 117401 42683   68.21137   50  289   79.00000   58.56113  25.9%  65.7   30s
 130135 44772   65.59470   51  254   79.00000   59.35650  24.9%  70.8   35s
 140770 46905   77.79701   59  300   79.00000   59.94053  24.1%  74.9   40s
 154501 48607   63.21928   43  341   79.00000   60.57072  23.3%  79.2   45s
 166067 52865   73.37287   58  304   79.00000   61.02995  22.7%  82.5   50s
 178222 58008     cutoff   60        79.00000   61.46373  22.2%  85.5   55s
 190142 63309   71.05340   56  278   79.00000   61.86889  21.7%  88.6   60s
 202843 68371     cutoff   54        79.00000   62.19453  21.3%  91.1   65s
 209880 71094   77.28471   56  256   79.00000   62.34406  21.1%  92.7   70s
 220061 75634   68.49392   52  289   79.00000   62.61175  20.7%  94.6   75s
 231448 80392   78.12607   51  225   79.00000   62.96045  20.3%  96.6   80s
 241693 84378   73.21902   53  227   79.00000   63.20529  20.0%  98.2   85s
 253927 88906   75.87079   55  233   79.00000   63.48547  19.6%   100   90s
 264145 92812   73.44973   49  222   79.00000   63.69046  19.4%   102   95s
 270780 94511   75.41565   51  311   79.00000   63.82233  19.2%   103  101s
 273975 95952   72.05181   52  320   79.00000   63.85348  19.2%   103  105s
 279273 98150     cutoff   63        79.00000   63.92581  19.1%   104  110s
 289892 102011   74.54719   54  275   79.00000   64.22901  18.7%   105  115s
 301556 105787   75.29367   53  240   79.00000   64.47443  18.4%   106  120s
 311543 108797   70.63184   52  219   79.00000   64.66536  18.1%   108  125s
 321842 112400   72.83440   55  322   79.00000   64.86220  17.9%   109  130s
 332247 115640   74.00226   54  319   79.00000   65.03872  17.7%   110  135s
 341985 118642   75.90535   51  298   79.00000   65.21888  17.4%   111  140s
 352635 121565   72.92663   48  330   79.00000   65.40264  17.2%   112  145s
 360793 124175   73.85280   53  320   79.00000   65.51225  17.1%   113  150s
 370835 126744   72.49858   55  315   79.00000   65.68232  16.9%   114  155s
 380572 129388     cutoff   52        79.00000   65.84402  16.7%   115  160s
 390647 131885   78.84678   65  219   79.00000   66.00050  16.5%   116  165s
 398325 133905     cutoff   55        79.00000   66.11144  16.3%   117  170s
 408085 136643   76.16190   50  276   79.00000   66.26469  16.1%   118  175s
 418571 139227   72.40152   53  320   79.00000   66.40643  15.9%   119  181s
 426541 140908   70.68745   55  310   79.00000   66.52523  15.8%   120  185s
 434198 142791   72.05611   48  331   79.00000   66.63370  15.7%   120  190s
 444494 144978     cutoff   62        79.00000   66.77355  15.5%   121  195s
 452544 146716     cutoff   55        79.00000   66.88471  15.3%   122  200s
 461107 148801   72.73229   51  326   79.00000   66.98801  15.2%   123  205s
 471051 151007   74.99268   52  294   79.00000   67.12875  15.0%   124  211s
 478957 152772   74.53650   51  316   79.00000   67.23849  14.9%   124  215s
 487777 154630   77.87624   52  299   79.00000   67.34190  14.8%   125  220s
 495887 156197   72.45363   48  331   79.00000   67.44935  14.6%   125  225s
 503272 157685   76.58791   54  315   79.00000   67.54047  14.5%   126  230s
 511882 159510   75.65689   56  283   79.00000   67.64211  14.4%   126  235s
 520322 161057   74.46412   53  281   79.00000   67.75056  14.2%   127  240s
 528171 162594   75.49326   58  318   79.00000   67.85067  14.1%   128  245s
 536258 164081   73.26022   50  314   79.00000   67.95861  14.0%   128  250s
 544062 165451   76.65703   56  268   79.00000   68.06388  13.8%   129  255s
 552289 166789   71.20299   49  332   79.00000   68.16069  13.7%   129  260s
 560832 168274   75.45064   50  284   79.00000   68.26654  13.6%   130  265s
 569059 169646     cutoff   51        79.00000   68.36697  13.5%   130  270s
 576912 171212   74.23392   53  268   79.00000   68.46812  13.3%   131  275s
 585378 172740   75.78334   55  322   79.00000   68.55737  13.2%   131  281s
 591836 173643   73.27758   51  333   79.00000   68.62113  13.1%   132  285s
 601624 174937   69.76473   46  243   79.00000   68.72916  13.0%   132  291s
 607789 175771   75.11454   56  225   79.00000   68.79785  12.9%   133  295s
 615935 176941   77.12236   54  271   79.00000   68.88679  12.8%   133  300s
 624088 177996   73.52500   54  294   79.00000   68.97311  12.7%   134  305s
 632076 179136     cutoff   51        79.00000   69.06002  12.6%   134  310s
 637915 179997   78.43274   56  248   79.00000   69.11442  12.5%   135  315s
 645496 181004     cutoff   55        79.00000   69.20272  12.4%   135  320s
 653684 182230   76.91972   63  253   79.00000   69.29163  12.3%   136  326s
 659790 182981     cutoff   55        79.00000   69.36135  12.2%   136  330s
 667807 183637   73.98068   51  279   79.00000   69.45683  12.1%   136  335s
 668291 184271   77.09132   58  268   79.00000   69.45866  12.1%   136  340s
 677931 184831   75.15753   57  303   79.00000   69.57703  11.9%   137  345s
 685015 185493   76.48586   57  281   79.00000   69.65174  11.8%   138  350s
 692831 186158   75.51969   50  289   79.00000   69.74830  11.7%   138  356s
 699610 186689   73.13341   48  333   79.00000   69.81782  11.6%   139  360s
 707716 187242   78.08561   53  242   79.00000   69.90984  11.5%   139  366s
 713479 187592   78.10165   57  247   79.00000   69.98181  11.4%   139  370s
 720916 188014   78.62710   56  246   79.00000   70.06684  11.3%   140  376s
 727192 188318   77.86033   54  319   79.00000   70.13649  11.2%   140  381s
 733440 188663   76.73541   54  315   79.00000   70.21333  11.1%   141  386s
 739575 188804     cutoff   50        79.00000   70.28615  11.0%   141  390s
 745230 189009   75.60513   53  309   79.00000   70.35596  10.9%   142  395s
 751630 189307   75.49684   54  322   79.00000   70.42738  10.9%   142  400s
 758172 189580     cutoff   52        79.00000   70.49604  10.8%   142  405s
 764975 189842     cutoff   53        79.00000   70.57373  10.7%   143  411s
 769653 189965   78.12518   53  282   79.00000   70.62807  10.6%   143  415s
 776257 190228     cutoff   57        79.00000   70.69472  10.5%   143  420s
 782295 190277   76.51861   54  257   79.00000   70.77131  10.4%   144  425s
 789864 190377     cutoff   59        79.00000   70.86051  10.3%   144  431s
 796028 190450   73.63428   48  332   79.00000   70.93024  10.2%   145  436s
 800401 190541     cutoff   47        79.00000   70.97723  10.2%   145  440s
 806573 190613     cutoff   60        79.00000   71.04557  10.1%   145  445s
 814104 190710   75.57225   49  244   79.00000   71.13305  10.0%   146  451s
 820121 190664   78.23751   57  172   79.00000   71.20252  9.87%   146  456s
 826387 190693   75.59182   55  258   79.00000   71.27335  9.78%   146  461s
 832147 190632     cutoff   59        79.00000   71.34382  9.69%   147  466s
 837843 190583   75.12960   49  318   79.00000   71.40114  9.62%   147  471s
 843444 190441   77.76185   53  286   79.00000   71.46734  9.54%   147  476s
 849698 190217   76.44121   53  311   79.00000   71.53251  9.45%   148  481s
 853801 190139   74.73246   55  320   79.00000   71.58460  9.39%   148  485s
 859658 189823     cutoff   52        79.00000   71.65535  9.30%   148  490s
 863743 189656   77.34505   52  276   79.00000   71.70841  9.23%   149  496s
 865467 189402   78.24688   54  264   79.00000   71.71175  9.23%   149  500s
 868631 189031     cutoff   56        79.00000   71.72812  9.20%   149  505s
 875416 188725   76.61204   59  304   79.00000   71.84712  9.05%   149  511s
 879909 188441     cutoff   52        79.00000   71.89802  8.99%   150  515s
 885978 187971     cutoff   53        79.00000   71.97773  8.89%   150  521s
 890298 187584   78.07241   51  267   79.00000   72.03016  8.82%   150  525s
 896551 186967     cutoff   52        79.00000   72.10551  8.73%   151  531s
 902576 186380     cutoff   58        79.00000   72.17854  8.63%   151  537s
 906559 185958   75.10543   50  331   79.00000   72.22981  8.57%   152  540s
 912654 185260     cutoff   55        79.00000   72.29805  8.48%   152  546s
 916657 184754   75.16154   52  306   79.00000   72.35017  8.42%   152  550s
 922559 184038   76.72543   55  307   79.00000   72.42884  8.32%   153  556s
 926474 183419     cutoff   57        79.00000   72.48002  8.25%   153  561s
 930367 182838     cutoff   57        79.00000   72.53012  8.19%   153  565s
 936185 181876     cutoff   55        79.00000   72.60841  8.09%   154  571s
 940202 181106     cutoff   54        79.00000   72.66194  8.02%   154  576s
 944069 180327     cutoff   53        79.00000   72.71562  7.95%   154  580s
 949793 179155     cutoff   53        79.00000   72.79233  7.86%   155  587s
 953862 178695   76.65498   53  297   79.00000   72.85285  7.78%   155  592s
 954949 178067   77.06045   55  296   79.00000   72.85835  7.77%   155  597s
 958762 177098     cutoff   56        79.00000   72.92547  7.69%   155  602s
 963211 176155   75.91747   54  222   79.00000   72.98963  7.61%   156  607s
 967750 175195   75.53202   54  295   79.00000   73.05295  7.53%   156  612s
 971764 174214   77.51928   55  293   79.00000   73.10802  7.46%   156  616s
 975476 173171     cutoff   46        79.00000   73.16601  7.38%   157  621s
 979585 172122     cutoff   53        79.00000   73.22647  7.31%   157  626s
 983932 171084     cutoff   55        79.00000   73.28841  7.23%   157  631s
 988044 169928   76.38637   52  309   79.00000   73.34339  7.16%   158  635s
 992077 168794     cutoff   58        79.00000   73.40607  7.08%   158  640s
 996087 167579     cutoff   50        79.00000   73.46644  7.00%   158  645s
 1000183 166391     cutoff   57        79.00000   73.52560  6.93%   158  650s
 1005943 164621     cutoff   50        79.00000   73.61514  6.82%   159  657s
 1009952 163298   78.59239   59  289   79.00000   73.67084  6.75%   159  662s
 1013981 161974     cutoff   55        79.00000   73.72978  6.67%   159  667s
 1018134 160580   75.52584   60  261   79.00000   73.79822  6.58%   160  672s
 1022164 159150   76.36405   51  259   79.00000   73.85716  6.51%   160  676s
 1025985 157591   78.56775   49  288   79.00000   73.92005  6.43%   160  681s
 1029745 156006     cutoff   64        79.00000   73.98686  6.35%   161  687s
 1033650 154431   77.46692   59  321   79.00000   74.05096  6.26%   161  692s
 1037466 152698   77.96105   51  326   79.00000   74.11246  6.19%   161  697s
 1039595 151873     cutoff   55        79.00000   74.15446  6.13%   161  700s
 1043572 150118     cutoff   59        79.00000   74.22089  6.05%   162  705s
 1047752 148310     cutoff   49        79.00000   74.28596  5.97%   162  710s
 1051450 146488     cutoff   53        79.00000   74.34830  5.89%   162  715s
 1055596 144489   77.47524   49  250   79.00000   74.42033  5.80%   162  721s
 1059718 142466     cutoff   62        79.00000   74.49251  5.71%   163  726s
 1063628 140385     cutoff   55        79.00000   74.56263  5.62%   163  731s
 1067691 138290     cutoff   66        79.00000   74.63239  5.53%   163  737s
 1071623 136013 infeasible   51        79.00000   74.69692  5.45%   164  742s
 1073694 134898     cutoff   63        79.00000   74.73676  5.40%   164  745s
 1077735 132694     cutoff   57        79.00000   74.80266  5.31%   164  750s
 1082051 130442     cutoff   51        79.00000   74.87327  5.22%   164  755s
 1085801 128018   77.85555   57  243   79.00000   74.94215  5.14%   164  761s
 1089876 125529     cutoff   58        79.00000   75.01088  5.05%   165  766s
 1094010 123148     cutoff   52        79.00000   75.08593  4.95%   165  771s
 1097745 120413   77.15435   55  261   79.00000   75.15578  4.87%   165  777s
 1101839 117758     cutoff   59        79.00000   75.23278  4.77%   165  782s
 1103975 116335     cutoff   57        79.00000   75.27128  4.72%   165  785s
 1108216 113570   78.46689   56  239   79.00000   75.34293  4.63%   166  790s
 1112297 110679     cutoff   56        79.00000   75.42495  4.53%   166  795s
 1116278 107728     cutoff   60        79.00000   75.50391  4.43%   166  800s
 1120474 104678     cutoff   55        79.00000   75.57790  4.33%   166  806s
 1124323 101746     cutoff   53        79.00000   75.65784  4.23%   166  811s
 1128504 98532     cutoff   59        79.00000   75.73562  4.13%   167  816s
 1132662 95356     cutoff   59        79.00000   75.81727  4.03%   167  821s
 1137062 92067     cutoff   58        79.00000   75.89531  3.93%   167  826s
 1141089 88729     cutoff   48        79.00000   75.97569  3.83%   167  831s
 1145129 85356     cutoff   56        79.00000   76.05756  3.72%   167  837s
 1149076 81966     cutoff   53        79.00000   76.13154  3.63%   167  842s
 1153286 78194   78.54376   52  320   79.00000   76.22251  3.52%   167  847s
 1157558 74490     cutoff   62        79.00000   76.30654  3.41%   167  852s
 1161835 70590     cutoff   60        79.00000   76.40120  3.29%   167  857s
 1166034 66615     cutoff   57        79.00000   76.48648  3.18%   167  862s
 1169160 64272     cutoff   54        79.00000   76.54751  3.10%   167  867s
 1170718 62400 infeasible   55        79.00000   76.54952  3.10%   167  870s
 1174799 57984     cutoff   60        79.00000   76.58742  3.05%   167  875s
 1179773 53148     cutoff   56        79.00000   76.77438  2.82%   167  880s
 1187094 46233     cutoff   56        79.00000   76.93705  2.61%   167  886s
 1191904 41646     cutoff   53        79.00000   77.05119  2.47%   167  890s
 1199031 34427     cutoff   48        79.00000   77.23005  2.24%   167  896s
 1203986 29500     cutoff   55        79.00000   77.35597  2.08%   167  900s
 1211435 21997     cutoff   58        79.00000   77.48423  1.92%   166  905s
 1222302 10706     cutoff   51        79.00000   77.90972  1.38%   165  910s

Cutting planes:
  Learned: 2
  Gomory: 97
  Cover: 1065
  Dual implied bound: 20
  MIR: 412
  Mixing: 2
  StrongCG: 10
  Flow cover: 468
  GUB cover: 48
  Inf proof: 688
  Zero half: 3
  RLT: 1
  Relax-and-lift: 21

Explored 1236046 nodes (202798070 simplex iterations) in 913.13 seconds (3675.73 work units)
Thread count was 20 (of 20 available processors)

Solution count 4: 79 118 358 360 

Optimal solution found (tolerance 1.00e-04)
Best objective 7.900000000000e+01, best bound 7.900000000000e+01, gap 0.0000%

H1 Solution:
  Objective: 79.000000
  Selected features: 6/6
  L1 norm: 0.000000
  Positive accuracy lb: 0.0000
  Negative accuracy lb: 1.0000

============================================================
Training H2: Class {Class 2, 3} (+1) vs Class 1 (-1)
============================================================
H2 Training samples: 120
  Positive (+1): 81 samples
  Negative (-1): 39 samples

Set parameter TimeLimit to value 1800
Set parameter MIPGap to value 0.0001
Set parameter OutputFlag to value 1
Set parameter Threads to value 0
Gurobi Optimizer version 13.0.1 build v13.0.1rc0 (linux64 - "Ubuntu 24.04.3 LTS")

CPU model: AMD Ryzen 9 9900X 12-Core Processor, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 20 physical cores, 20 logical processors, using up to 20 threads

Non-default parameters:
TimeLimit  1800

Optimize a model with 854 rows, 501 columns and 3278 nonzeros (Min)
Model fingerprint: 0xdea831e5
Model has 374 linear objective coefficients
Variable types: 135 continuous, 366 integer (366 binary)
Coefficient statistics:
  Matrix range     [1e-07, 1e+03]
  Objective range  [1e+00, 1e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 2e+02]

Found heuristic solution: objective 77.0000000
Presolve removed 205 rows and 121 columns
Presolve time: 0.00s
Presolved: 649 rows, 380 columns, 2378 nonzeros
Variable types: 12 continuous, 368 integer (276 binary)

Root relaxation: objective -1.803000e+00, 275 iterations, 0.00 seconds (0.01 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0   -1.80300    0   90   77.00000   -1.80300   102%     -    0s
H    0     0                      10.0000000   -1.80300   118%     -    0s
H    0     0                       0.0000000   -1.80300      -     -    0s
H    0     0                      -0.0000000   -1.80300      -     -    0s

Cutting planes:
  Gomory: 9
  Implied bound: 30

Explored 1 nodes (275 simplex iterations) in 0.01 seconds (0.02 work units)
Thread count was 20 (of 20 available processors)

Solution count 3: -1e-10 10 77 

Optimal solution found (tolerance 1.00e-04)
Best objective -1.000000082740e-10, best bound -1.000000082740e-10, gap 0.0000%

H2 Solution:
  Objective: -0.000000
  Selected features: 6/6
  L1 norm: 2.000000
  Positive accuracy lb: 1.0000
  Negative accuracy lb: 1.0000

============================================================
Training Complete!
============================================================

Training completed in: 0:15:13.157425

================================================================================
Model Summary
================================================================================
Status: fitted
Strategy: inverted
Features: 6

Class Roles:
  Majority: Class 3
  Medium:   Class 2
  Minority: Class 1

Classifier 1 (H1): Class 2 vs {Class 3, 1}
  Objective: 79.000000
  Selected Features: 6/6
  L1 Norm: 0.000000

Classifier 2 (H2): Class {Class 2, 3} vs Class 1
  Objective: -0.000000
  Selected Features: 6/6
  L1 Norm: 2.000000

================================================================================
Training Set Evaluation
================================================================================


============================================================
Evaluation Results
============================================================

Total Accuracy: 0.6667

Per-Class Accuracy:
  Class 1: 1.0000
  Class 2: 0.0000
  Class 3: 1.0000

Class Distribution:
  Class 1: 39 samples
  Class 2: 40 samples
  Class 3: 41 samples

Confusion Matrix:
     Pred 1  Pred 2  Pred 3
True 1:    39       0       0
True 2:     0       0      40
True 3:     0       0      41
============================================================

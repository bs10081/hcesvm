================================================================================
Hierarchical CE-SVM - Inverted Strategy
Dataset: Balance
Timestamp: 2026-01-31 13:01:22
================================================================================

Loading training data from: /home/bs10081/Developer/NSVORA/Archive/balance_split.xlsx

Loaded multi-class data:
  Class 1: 230 samples
  Class 2: 39 samples
  Class 3: 231 samples
  Features: 4
Training data loaded:
  Class 1: 230 samples
  Class 2: 39 samples
  Class 3: 231 samples
  Features: 4

================================================================================
Model Parameters
================================================================================
  C_hyper: 1.0
  epsilon: 0.0001
  M: 1000.0
  time_limit: 1800
  mip_gap: 0.0001
  threads: 0
  verbose: True
  enable_selection: True
  feat_upper_bound: 1000
  feat_lower_bound: 1e-07

================================================================================
Training Hierarchical Classifier - Inverted Strategy
================================================================================

Inverted Strategy:
  H1: medium class vs {majority, minority}
  H2: {medium, majority} vs minority
  (Class roles determined dynamically based on sample counts)

============================================================
Training Hierarchical CE-SVM (Strategy: inverted)
============================================================
Class 1: 230 samples
Class 2: 39 samples
Class 3: 231 samples
Features: 4

Dynamic Class Roles:
  Majority:  Class 3 (231 samples)
  Medium:    Class 1 (230 samples)
  Minority:  Class 2 (39 samples)

============================================================
Training H1: Class 1 (+1) vs {Class 3, 2} (-1)
============================================================
H1 Training samples: 500
  Positive (+1): 230 samples
  Negative (-1): 270 samples

Set parameter TimeLimit to value 1800
Set parameter MIPGap to value 0.0001
Set parameter OutputFlag to value 1
Set parameter Threads to value 0
Gurobi Optimizer version 13.0.1 build v13.0.1rc0 (linux64 - "Ubuntu 24.04.3 LTS")

CPU model: AMD Ryzen 9 9900X 12-Core Processor, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 20 physical cores, 20 logical processors, using up to 20 threads

Non-default parameters:
TimeLimit  1800

Optimize a model with 3510 rows, 2015 columns and 11526 nonzeros (Min)
Model fingerprint: 0xaf6d524f
Model has 1510 linear objective coefficients
Variable types: 511 continuous, 1504 integer (1504 binary)
Coefficient statistics:
  Matrix range     [1e-07, 1e+03]
  Objective range  [1e+00, 1e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 5e+02]

Presolve removed 6 rows and 6 columns
Presolve time: 0.01s
Presolved: 3504 rows, 2009 columns, 11194 nonzeros
Variable types: 9 continuous, 2000 integer (1500 binary)
Found heuristic solution: objective 1500.0000000
Found heuristic solution: objective 1499.0000000
Found heuristic solution: objective 498.0000000

Root relaxation: objective -9.980801e-01, 1581 iterations, 0.03 seconds (0.15 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0   -0.99808    0 1500  498.00000   -0.99808   100%     -    0s
     0     0    0.44081    0 1355  498.00000    0.44081   100%     -    0s
     0     0    0.53836    0 1122  498.00000    0.53836   100%     -    0s
     0     0    0.56438    0 1140  498.00000    0.56438   100%     -    0s
     0     0    1.68962    0 1054  498.00000    1.68962   100%     -    0s
     0     0    1.95318    0 1242  498.00000    1.95318   100%     -    0s
     0     0    1.98720    0 1082  498.00000    1.98720   100%     -    0s
     0     0    1.98947    0 1072  498.00000    1.98947   100%     -    0s
H    0     0                     145.7257649    8.39951  94.2%     -    0s
H    0     0                     143.7257649    8.39951  94.2%     -    0s
     0     0    8.39951    0  323  143.72576    8.39951  94.2%     -    0s
H    0     0                      99.4663446    8.40361  91.6%     -    0s
     0     0    9.10862    0  377   99.46634    9.10862  90.8%     -    0s
     0     0    9.33264    0  323   99.46634    9.33264  90.6%     -    0s
     0     0    9.37153    0  322   99.46634    9.37153  90.6%     -    0s
     0     0    9.40890    0  322   99.46634    9.40890  90.5%     -    0s
     0     0    9.40909    0  327   99.46634    9.40909  90.5%     -    0s
     0     0   12.09573    0  357   99.46634   12.09573  87.8%     -    0s
     0     0   12.11533    0  394   99.46634   12.11533  87.8%     -    0s
     0     0   12.14129    0  373   99.46634   12.14129  87.8%     -    0s
     0     0   12.61596    0  427   99.46634   12.61596  87.3%     -    0s
     0     0   12.75941    0  353   99.46634   12.75941  87.2%     -    0s
     0     0   12.76456    0  363   99.46634   12.76456  87.2%     -    0s
     0     0   12.76823    0  375   99.46634   12.76823  87.2%     -    0s
     0     0   13.45526    0  440   99.46634   13.45526  86.5%     -    0s
     0     0   14.18007    0  380   99.46634   14.18007  85.7%     -    0s
     0     0   14.22340    0  394   99.46634   14.22340  85.7%     -    0s
     0     0   14.22472    0  403   99.46634   14.22472  85.7%     -    0s
     0     0   14.38807    0  402   99.46634   14.38807  85.5%     -    0s
     0     0   14.43417    0  420   99.46634   14.43417  85.5%     -    0s
     0     0   14.44445    0  400   99.46634   14.44445  85.5%     -    0s
     0     0   14.44560    0  405   99.46634   14.44560  85.5%     -    0s
     0     0   14.82041    0  493   99.46634   14.82041  85.1%     -    0s
     0     0   15.04431    0  432   99.46634   15.04431  84.9%     -    0s
H    0     0                      94.5813205   15.06146  84.1%     -    0s
     0     0   15.06146    0  445   94.58132   15.06146  84.1%     -    0s
H    0     0                      53.0900161   15.06492  71.6%     -    0s
     0     0   15.06492    0  450   53.09002   15.06492  71.6%     -    0s
H    0     0                      51.0856683   15.17247  70.3%     -    0s
H    0     0                      47.0813205   15.17247  67.8%     -    0s
     0     0   15.17247    0  415   47.08132   15.17247  67.8%     -    0s
     0     0   15.17661    0  412   47.08132   15.17661  67.8%     -    0s
     0     0   15.50559    0  481   47.08132   15.50559  67.1%     -    0s
     0     0   15.60757    0  438   47.08132   15.60757  66.8%     -    0s
     0     0   15.65026    0  462   47.08132   15.65026  66.8%     -    0s
     0     0   15.69781    0  443   47.08132   15.69781  66.7%     -    0s
     0     0   15.70336    0  442   47.08132   15.70336  66.6%     -    0s
     0     0   15.71273    0  451   47.08132   15.71273  66.6%     -    0s
     0     0   15.71934    0  464   47.08132   15.71934  66.6%     -    0s
     0     0   16.06596    0  452   47.08132   16.06596  65.9%     -    0s
H    0     0                      46.0813205   16.51950  64.2%     -    0s
     0     0   16.51950    0  379   46.08132   16.51950  64.2%     -    0s
     0     0   16.56630    0  384   46.08132   16.56630  64.0%     -    0s
     0     0   16.59767    0  388   46.08132   16.59767  64.0%     -    0s
     0     0   16.62328    0  364   46.08132   16.62328  63.9%     -    0s
     0     0   16.64860    0  355   46.08132   16.64860  63.9%     -    0s
     0     0   16.65697    0  393   46.08132   16.65697  63.9%     -    0s
     0     0   16.66164    0  392   46.08132   16.66164  63.8%     -    0s
     0     0   16.77103    0  379   46.08132   16.77103  63.6%     -    0s
     0     0   16.89839    0  369   46.08132   16.89839  63.3%     -    0s
     0     0   16.93873    0  363   46.08132   16.93873  63.2%     -    0s
     0     0   16.94344    0  383   46.08132   16.94344  63.2%     -    0s
     0     0   17.07349    0  342   46.08132   17.07349  62.9%     -    0s
     0     0   17.33044    0  363   46.08132   17.33044  62.4%     -    0s
     0     0   17.35166    0  347   46.08132   17.35166  62.3%     -    0s
     0     0   17.35259    0  342   46.08132   17.35259  62.3%     -    0s
     0     0   17.45796    0  395   46.08132   17.45796  62.1%     -    0s
     0     0   17.54834    0  374   46.08132   17.54834  61.9%     -    0s
     0     0   17.55216    0  400   46.08132   17.55216  61.9%     -    0s
     0     0   17.78154    0  379   46.08132   17.78154  61.4%     -    0s
     0     0   17.88157    0  356   46.08132   17.88157  61.2%     -    0s
     0     0   17.89641    0  360   46.08132   17.89641  61.2%     -    0s
     0     0   17.89882    0  349   46.08132   17.89882  61.2%     -    0s
     0     0   18.00378    0  315   46.08132   18.00378  60.9%     -    0s
     0     0   18.09133    0  352   46.08132   18.09133  60.7%     -    0s
     0     0   18.12597    0  354   46.08132   18.12597  60.7%     -    0s
     0     0   18.12649    0  353   46.08132   18.12649  60.7%     -    0s
     0     0   18.17998    0  328   46.08132   18.17998  60.5%     -    0s
     0     0   18.19688    0  358   46.08132   18.19688  60.5%     -    0s
     0     0   18.19959    0  366   46.08132   18.19959  60.5%     -    0s
     0     0   18.48283    0  349   46.08132   18.48283  59.9%     -    0s
     0     0   18.53551    0  363   46.08132   18.53551  59.8%     -    0s
     0     0   18.55295    0  361   46.08132   18.55295  59.7%     -    0s
     0     0   18.56651    0  371   46.08132   18.56651  59.7%     -    0s
     0     0   18.56830    0  365   46.08132   18.56830  59.7%     -    0s
     0     0   18.86704    0  407   46.08132   18.86704  59.1%     -    0s
     0     0   19.27085    0  379   46.08132   19.27085  58.2%     -    0s
     0     0   19.37635    0  382   46.08132   19.37635  58.0%     -    0s
     0     0   19.39722    0  379   46.08132   19.39722  57.9%     -    0s
     0     0   19.41372    0  370   46.08132   19.41372  57.9%     -    0s
     0     0   19.41824    0  402   46.08132   19.41824  57.9%     -    0s
     0     0   19.55291    0  391   46.08132   19.55291  57.6%     -    0s
     0     0   19.77457    0  367   46.08132   19.77457  57.1%     -    0s
     0     0   19.88494    0  371   46.08132   19.88494  56.8%     -    0s
     0     0   19.92802    0  377   46.08132   19.92802  56.8%     -    0s
     0     0   19.94042    0  383   46.08132   19.94042  56.7%     -    0s
     0     0   19.94277    0  376   46.08132   19.94277  56.7%     -    0s
     0     0   19.98927    0  379   46.08132   19.98927  56.6%     -    0s
     0     0   20.05704    0  398   46.08132   20.05704  56.5%     -    0s
     0     0   20.06260    0  389   46.08132   20.06260  56.5%     -    1s
     0     0   20.16145    0  384   46.08132   20.16145  56.2%     -    1s
     0     0   20.24382    0  385   46.08132   20.24382  56.1%     -    1s
     0     0   20.25501    0  387   46.08132   20.25501  56.0%     -    1s
     0     0   20.25979    0  386   46.08132   20.25979  56.0%     -    1s
     0     0   20.27664    0  386   46.08132   20.27664  56.0%     -    1s
     0     0   20.29404    0  396   46.08132   20.29404  56.0%     -    1s
     0     0   20.29866    0  403   46.08132   20.29866  56.0%     -    1s
     0     0   20.32802    0  374   46.08132   20.32802  55.9%     -    1s
     0     0   20.32873    0  374   46.08132   20.32873  55.9%     -    1s
     0     0   20.32873    0  374   46.08132   20.32873  55.9%     -    1s
     0     2   20.32908    0  374   46.08132   20.32908  55.9%     -    1s
H   57    72                      46.0813200   21.46449  53.4%   200    1s
  3724  2007   32.02326   17  423   46.08132   25.39948  44.9%  87.9    5s
H 4392  2191                      46.0813195   31.59494  31.4%  98.7    8s
  7483  2702   41.57701   28  525   46.08132   33.83128  26.6%  94.6   10s
 17494  5906   44.79580   29  481   46.08132   37.34482  19.0%  92.6   15s
 34705  5795   45.52229   28  302   46.08132   40.80339  11.5%  93.2   20s

Cutting planes:
  Learned: 1
  Cover: 46
  MIR: 218
  StrongCG: 1
  Flow cover: 328
  GUB cover: 1
  Inf proof: 118
  Zero half: 2
  RLT: 1

Explored 41274 nodes (3685626 simplex iterations) in 22.48 seconds (59.72 work units)
Thread count was 20 (of 20 available processors)

Solution count 10: 46.0813 46.0813 46.0813 ... 99.4663

Optimal solution found (tolerance 1.00e-04)
Best objective 4.608132045089e+01, best bound 4.608131946519e+01, gap 0.0000%

H1 Solution:
  Objective: 46.081320
  Selected features: 4/4
  L1 norm: 8.000000
  Positive accuracy lb: 0.9261
  Negative accuracy lb: 0.9926

============================================================
Training H2: Class {Class 1, 3} (+1) vs Class 2 (-1)
============================================================
H2 Training samples: 500
  Positive (+1): 461 samples
  Negative (-1): 39 samples

Set parameter TimeLimit to value 1800
Set parameter MIPGap to value 0.0001
Set parameter OutputFlag to value 1
Set parameter Threads to value 0
Gurobi Optimizer version 13.0.1 build v13.0.1rc0 (linux64 - "Ubuntu 24.04.3 LTS")

CPU model: AMD Ryzen 9 9900X 12-Core Processor, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 20 physical cores, 20 logical processors, using up to 20 threads

Non-default parameters:
TimeLimit  1800

Optimize a model with 3510 rows, 2015 columns and 11526 nonzeros (Min)
Model fingerprint: 0xf4e88ad4
Model has 1510 linear objective coefficients
Variable types: 511 continuous, 1504 integer (1504 binary)
Coefficient statistics:
  Matrix range     [1e-07, 1e+03]
  Objective range  [1e+00, 1e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 9e+02]

Found heuristic solution: objective 202.5000000
Presolve removed 6 rows and 6 columns
Presolve time: 0.01s
Presolved: 3504 rows, 2009 columns, 11194 nonzeros
Found heuristic solution: objective 202.0000000
Variable types: 9 continuous, 2000 integer (1500 binary)

Root relaxation: objective -1.803000e+00, 735 iterations, 0.01 seconds (0.04 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0   -1.80300    0  117  202.00000   -1.80300   101%     -    0s
H    0     0                      77.0000000   -1.80300   102%     -    0s
     0     0    0.19198    0  117   77.00000    0.19198   100%     -    0s
     0     0    3.34546    0  319   77.00000    3.34546  95.7%     -    0s
     0     0    3.41272    0  319   77.00000    3.41272  95.6%     -    0s
     0     0    3.41272    0  319   77.00000    3.41272  95.6%     -    0s
     0     0    9.78494    0  348   77.00000    9.78494  87.3%     -    0s
     0     0   10.36907    0  362   77.00000   10.36907  86.5%     -    0s
     0     0   10.40547    0  366   77.00000   10.40547  86.5%     -    0s
     0     0   10.40577    0  370   77.00000   10.40577  86.5%     -    0s
     0     0   14.84696    0  355   77.00000   14.84696  80.7%     -    0s
     0     0   15.28996    0  259   77.00000   15.28996  80.1%     -    0s
     0     0   15.47907    0  326   77.00000   15.47907  79.9%     -    0s
     0     0   15.49237    0  328   77.00000   15.49237  79.9%     -    0s
     0     0   15.50773    0  338   77.00000   15.50773  79.9%     -    0s
     0     0   15.51175    0  342   77.00000   15.51175  79.9%     -    0s
     0     0   15.51365    0  351   77.00000   15.51365  79.9%     -    0s
     0     0   16.84089    0  347   77.00000   16.84089  78.1%     -    0s
     0     0   17.05073    0  335   77.00000   17.05073  77.9%     -    0s
     0     0   17.20608    0  342   77.00000   17.20608  77.7%     -    0s
     0     0   17.21425    0  344   77.00000   17.21425  77.6%     -    0s
     0     0   17.21510    0  343   77.00000   17.21510  77.6%     -    0s
     0     0   17.31580    0  328   77.00000   17.31580  77.5%     -    0s
     0     0   17.65552    0  320   77.00000   17.65552  77.1%     -    0s
     0     0   17.67510    0  325   77.00000   17.67510  77.0%     -    0s
     0     0   17.70512    0  317   77.00000   17.70512  77.0%     -    0s
     0     0   17.71043    0  325   77.00000   17.71043  77.0%     -    0s
     0     0   17.71103    0  326   77.00000   17.71103  77.0%     -    0s
     0     0   18.03687    0  304   77.00000   18.03687  76.6%     -    0s
     0     0   18.12897    0  325   77.00000   18.12897  76.5%     -    0s
     0     0   18.24175    0  340   77.00000   18.24175  76.3%     -    0s
     0     0   18.24612    0  338   77.00000   18.24612  76.3%     -    0s
     0     0   18.24941    0  335   77.00000   18.24941  76.3%     -    0s
     0     0   18.29123    0  323   77.00000   18.29123  76.2%     -    0s
     0     0   18.31615    0  311   77.00000   18.31615  76.2%     -    0s
     0     0   18.31856    0  321   77.00000   18.31856  76.2%     -    0s
     0     0   18.39464    0  327   77.00000   18.39464  76.1%     -    0s
     0     0   18.40960    0  338   77.00000   18.40960  76.1%     -    0s
     0     0   18.41097    0  341   77.00000   18.41097  76.1%     -    0s
     0     0   18.57010    0  321   77.00000   18.57010  75.9%     -    0s
     0     0   18.60031    0  325   77.00000   18.60031  75.8%     -    0s
     0     0   18.60690    0  346   77.00000   18.60690  75.8%     -    0s
     0     0   18.62977    0  344   77.00000   18.62977  75.8%     -    0s
     0     0   18.64527    0  336   77.00000   18.64527  75.8%     -    0s
     0     0   18.64863    0  340   77.00000   18.64863  75.8%     -    0s
     0     0   18.65724    0  345   77.00000   18.65724  75.8%     -    0s
     0     0   18.66028    0  357   77.00000   18.66028  75.8%     -    0s
     0     0   18.67498    0  309   77.00000   18.67498  75.7%     -    0s
     0     0   18.67644    0  334   77.00000   18.67644  75.7%     -    0s
     0     0   18.67676    0  327   77.00000   18.67676  75.7%     -    0s
     0     0   18.68653    0  327   77.00000   18.68653  75.7%     -    0s
     0     0   18.68653    0  326   77.00000   18.68653  75.7%     -    0s
     0     2   18.68680    0  326   77.00000   18.68680  75.7%     -    0s
  3087  1923   43.03671   21 1846   77.00000   36.99202  52.0%   102    5s
  7156  3437   48.89967   25 1724   77.00000   39.39169  48.8%   102   10s
 11737  5588   52.26821   28 1591   77.00000   41.72464  45.8%  94.5   15s
 20796 11124   55.54044   32 1487   77.00000   44.23300  42.6%  89.2   20s
 23389 11997   59.22612   41 1249   77.00000   54.71497  28.9%  91.9   25s
 25802 12720   69.96937   51 1409   77.00000   57.66486  25.1%   101   30s
 35927 15651   72.17424   40 1888   77.00000   61.84843  19.7%   110   35s
 44528 16180   73.70917   39 1764   77.00000   63.75575  17.2%   114   40s
 53704 16218   75.24899   46 1031   77.00000   65.21543  15.3%   119   45s
 62041 16809     cutoff   45        77.00000   66.09382  14.2%   122   50s
 69551 18185   74.00938   45  953   77.00000   66.94172  13.1%   126   60s
 79257 19091   74.80122   50 1136   77.00000   68.07369  11.6%   131   66s
 83077 18901   75.19800   44 1584   77.00000   68.78041  10.7%   134   70s
 88522 14399     cutoff   52        77.00000   71.70951  6.87%   142   75s
 92752  9733     cutoff   41        77.00000   73.41585  4.65%   149   80s
 98455  3371     cutoff   43        77.00000   74.79633  2.86%   154   85s

Cutting planes:
  Learned: 2
  Gomory: 16
  Cover: 93
  Dual implied bound: 5
  MIR: 238
  StrongCG: 1
  Flow cover: 309
  GUB cover: 5
  Inf proof: 201
  Zero half: 6
  Relax-and-lift: 2

Explored 104185 nodes (15827391 simplex iterations) in 86.89 seconds (275.33 work units)
Thread count was 20 (of 20 available processors)

Solution count 3: 77 202 202.5 

Optimal solution found (tolerance 1.00e-04)
Best objective 7.700000000000e+01, best bound 7.700000000000e+01, gap 0.0000%

H2 Solution:
  Objective: 77.000000
  Selected features: 4/4
  L1 norm: 0.000000
  Positive accuracy lb: 1.0000
  Negative accuracy lb: 0.0000

============================================================
Training Complete!
============================================================

Training completed in: 0:01:49.456936

================================================================================
Model Summary
================================================================================
Status: fitted
Strategy: inverted
Features: 4

Class Roles:
  Majority: Class 3
  Medium:   Class 1
  Minority: Class 2

Classifier 1 (H1): Class 1 vs {Class 3, 2}
  Objective: 46.081320
  Selected Features: 4/4
  L1 Norm: 8.000000

Classifier 2 (H2): Class {Class 1, 3} vs Class 2
  Objective: 77.000000
  Selected Features: 4/4
  L1 Norm: 0.000000

================================================================================
Training Set Evaluation
================================================================================


============================================================
Evaluation Results
============================================================

Total Accuracy: 0.8860

Per-Class Accuracy:
  Class 1: 0.9261
  Class 2: 0.0000
  Class 3: 0.9957

Class Distribution:
  Class 1: 230 samples
  Class 2: 39 samples
  Class 3: 231 samples

Confusion Matrix:
     Pred 1  Pred 2  Pred 3
True 1:   213       0      17
True 2:     1       0      38
True 3:     1       0     230
============================================================

================================================================================
Test Set Evaluation
================================================================================

Loading test data (skiprows=4)...
Loaded multi-class data:
  Class 1: 58 samples
  Class 2: 10 samples
  Class 3: 57 samples
  Features: 4
Test data loaded:
  Class 1: 58 samples
  Class 2: 10 samples
  Class 3: 57 samples


============================================================
Evaluation Results
============================================================

Total Accuracy: 0.8400

Per-Class Accuracy:
  Class 1: 0.8793
  Class 2: 0.0000
  Class 3: 0.9474

Class Distribution:
  Class 1: 58 samples
  Class 2: 10 samples
  Class 3: 57 samples

Confusion Matrix:
     Pred 1  Pred 2  Pred 3
True 1:    51       0       7
True 2:     1       0       9
True 3:     3       0      54
============================================================

================================================================================
Test Complete!
================================================================================
Training Duration: 0:01:49.456936
Training Accuracy: 0.8860
Test Accuracy: 0.8400
Log saved to: /home/bs10081/Developer/hcesvm/results/inverted_Balance_20260131_130122.log
================================================================================

================================================================================
Hierarchical CE-SVM - Inverted Strategy
Dataset: TAE
Timestamp: 2026-01-31 13:33:12
================================================================================

Loading training data from: /home/bs10081/Developer/NSVORA/Archive/tae_split.xlsx

Loaded multi-class data:
  Class 1: 39 samples
  Class 2: 40 samples
  Class 3: 41 samples
  Features: 6
Training data loaded:
  Class 1: 39 samples
  Class 2: 40 samples
  Class 3: 41 samples
  Features: 6

================================================================================
Model Parameters
================================================================================
  C_hyper: 1.0
  epsilon: 0.0001
  M: 1000.0
  time_limit: 1800
  mip_gap: 0.0001
  threads: 0
  verbose: True
  enable_selection: True
  feat_upper_bound: 1000
  feat_lower_bound: 1e-07

================================================================================
Training Hierarchical Classifier - Inverted Strategy
================================================================================

Inverted Strategy:
  H1: medium class vs {majority, minority}
  H2: {medium, majority} vs minority
  (Class roles determined dynamically based on sample counts)

============================================================
Training Hierarchical CE-SVM (Strategy: inverted)
============================================================
Class 1: 39 samples
Class 2: 40 samples
Class 3: 41 samples
Features: 6

Dynamic Class Roles:
  Majority:  Class 3 (41 samples)
  Medium:    Class 2 (40 samples)
  Minority:  Class 1 (39 samples)

============================================================
Training H1: Class 2 (+1) vs {Class 3, 1} (-1)
============================================================
H1 Training samples: 120
  Positive (+1): 40 samples
  Negative (-1): 80 samples

Set parameter TimeLimit to value 1800
Set parameter MIPGap to value 0.0001
Set parameter OutputFlag to value 1
Set parameter Threads to value 0
Gurobi Optimizer version 13.0.1 build v13.0.1rc0 (linux64 - "Ubuntu 24.04.3 LTS")

CPU model: AMD Ryzen 9 9900X 12-Core Processor, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 20 physical cores, 20 logical processors, using up to 20 threads

Non-default parameters:
TimeLimit  1800

Optimize a model with 854 rows, 501 columns and 3278 nonzeros (Min)
Model fingerprint: 0xc8d15649
Model has 374 linear objective coefficients
Variable types: 135 continuous, 366 integer (366 binary)
Coefficient statistics:
  Matrix range     [1e-07, 1e+03]
  Objective range  [1e+00, 1e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 2e+02]

Presolve removed 204 rows and 120 columns
Presolve time: 0.00s
Presolved: 650 rows, 381 columns, 2442 nonzeros
Variable types: 13 continuous, 368 integer (276 binary)
Found heuristic solution: objective 360.0000000
Found heuristic solution: objective 358.0000000
Found heuristic solution: objective 118.0000000

Root relaxation: objective -1.798000e+00, 279 iterations, 0.00 seconds (0.01 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0   -1.79800    0   93  118.00000   -1.79800   102%     -    0s
H    0     0                      79.0000000   -1.79800   102%     -    0s
     0     0    0.39880    0  217   79.00000    0.39880  99.5%     -    0s
     0     0    1.17667    0  225   79.00000    1.17667  98.5%     -    0s
     0     0    2.24492    0  184   79.00000    2.24492  97.2%     -    0s
     0     0    2.24492    0  195   79.00000    2.24492  97.2%     -    0s
     0     0    2.24492    0  195   79.00000    2.24492  97.2%     -    0s
     0     0    4.34526    0  242   79.00000    4.34526  94.5%     -    0s
     0     0    4.46346    0  227   79.00000    4.46346  94.4%     -    0s
     0     0    4.60009    0  230   79.00000    4.60009  94.2%     -    0s
     0     0    4.64683    0  236   79.00000    4.64683  94.1%     -    0s
     0     0    6.43793    0  223   79.00000    6.43793  91.9%     -    0s
     0     0    6.44648    0  223   79.00000    6.44648  91.8%     -    0s
     0     0    6.44648    0  223   79.00000    6.44648  91.8%     -    0s
     0     2    6.44648    0  223   79.00000    6.44648  91.8%     -    0s
 48574 21869   56.50213   38  157   79.00000   46.28466  41.4%  34.5    5s
 69732 29380   57.76509   44  258   79.00000   48.71831  38.3%  37.4   10s
 72357 30619   58.49778   43  326   79.00000   50.80629  35.7%  40.5   15s
 87717 35379   63.49505   48  261   79.00000   55.48354  29.8%  50.1   20s
 102012 39350   68.63781   57  275   79.00000   57.23233  27.6%  58.0   25s
 115791 42251   62.15121   49  313   79.00000   58.44992  26.0%  65.0   30s
 130135 44772   65.59470   51  254   79.00000   59.35650  24.9%  70.8   35s
 140770 46905   77.79701   59  300   79.00000   59.94053  24.1%  74.9   40s
 154501 48607   63.21928   43  341   79.00000   60.57072  23.3%  79.2   45s
 166067 52865   73.37287   58  304   79.00000   61.02995  22.7%  82.5   50s
 178222 58008     cutoff   60        79.00000   61.46373  22.2%  85.5   55s
 188140 62486   71.15509   55  315   79.00000   61.80031  21.8%  88.1   60s
 200926 67687   66.40203   50  327   79.00000   62.13893  21.3%  90.6   65s
 208771 70518   78.55467   53  302   79.00000   62.33975  21.1%  92.4   70s
 217813 74779   65.48775   50  313   79.00000   62.52706  20.9%  94.1   75s
 229312 79487   70.69045   48  317   79.00000   62.91152  20.4%  96.2   80s
 239630 83613   66.93919   51  312   79.00000   63.14438  20.1%  97.9   85s
 252100 88052   77.18111   58  248   79.00000   63.43492  19.7%   100   90s
 261980 92127   75.55393   48  314   79.00000   63.64782  19.4%   101   95s
 270477 94308   71.98575   50  309   79.00000   63.82233  19.2%   102  101s
 272934 95504   76.73476   58  280   79.00000   63.84130  19.2%   103  105s
 279273 98150     cutoff   63        79.00000   63.92581  19.1%   104  111s
 287410 101160   75.83734   47  264   79.00000   64.12396  18.8%   105  115s
 297185 104248     cutoff   56        79.00000   64.38823  18.5%   106  120s
 310000 108209   69.26847   48  322   79.00000   64.62982  18.2%   107  125s
 317577 111073   75.45388   63  285   79.00000   64.77405  18.0%   108  130s
 328080 114413   68.47022   51  332   79.00000   64.96805  17.8%   110  135s
 338164 117408   75.16437   48  284   79.00000   65.14890  17.5%   111  140s
 348172 120407   77.13826   59  278   79.00000   65.32520  17.3%   112  145s
 358825 123520   69.14491   45  297   79.00000   65.49490  17.1%   113  150s
 368355 126299   78.77633   48  247   79.00000   65.65151  16.9%   114  155s
 376520 128480   73.01650   52  268   79.00000   65.78460  16.7%   115  160s
 386539 130940   78.44925   63  267   79.00000   65.93539  16.5%   116  165s
 395861 133462   71.03438   49  333   79.00000   66.08457  16.3%   117  170s
 404025 135563   77.29739   57  261   79.00000   66.19726  16.2%   117  175s
 414309 138246   76.57789   47  253   79.00000   66.35129  16.0%   118  180s
 422557 140090   78.85119   52  278   79.00000   66.46423  15.9%   119  185s
 432342 142274   76.47910   57  310   79.00000   66.60897  15.7%   120  191s
 440501 144058   75.63946   50  326   79.00000   66.71367  15.6%   121  195s
 448665 145879 infeasible   63        79.00000   66.82537  15.4%   122  200s
 456449 147809     cutoff   60        79.00000   66.93729  15.3%   122  205s
 465300 149622   75.18084   54  316   79.00000   67.04587  15.1%   123  210s
 475279 151844   73.04288   53  329   79.00000   67.18305  15.0%   124  215s
 483238 153822     cutoff   62        79.00000   67.28894  14.8%   124  220s
 491811 155496   73.23694   46  319   79.00000   67.39437  14.7%   125  225s
 499828 156890     cutoff   59        79.00000   67.50049  14.6%   126  230s
 507331 158709   72.72832   51  297   79.00000   67.58877  14.4%   126  235s
 516082 160252   76.00195   50  226   79.00000   67.69407  14.3%   127  240s
 526190 162248   72.83610   48  327   79.00000   67.82274  14.1%   127  246s
 534303 163631   72.56195   47  261   79.00000   67.93770  14.0%   128  251s
 540145 164686   71.94574   52  281   79.00000   68.01384  13.9%   129  255s
 548693 166070   76.52719   54  305   79.00000   68.11480  13.8%   129  260s
 556533 167571   73.53190   52  278   79.00000   68.21497  13.7%   130  265s
 565069 168932     cutoff   53        79.00000   68.32531  13.5%   130  270s
 573168 170422   76.99049   54  322   79.00000   68.42179  13.4%   131  275s
 581174 172030   75.87305   50  323   79.00000   68.51625  13.3%   131  281s
 589542 173263   78.55644   60  244   79.00000   68.60518  13.2%   132  286s
 597779 174380     cutoff   57        79.00000   68.68555  13.1%   132  291s
 603490 175191     cutoff   54        79.00000   68.75201  13.0%   133  295s
 611880 176321     cutoff   58        79.00000   68.84388  12.9%   133  300s
 620333 177385     cutoff   53        79.00000   68.93117  12.7%   134  305s
 628015 178592   73.99585   53  315   79.00000   69.01701  12.6%   134  310s
 636117 179742     cutoff   52        79.00000   69.09770  12.5%   135  316s
 641903 180544   77.89747   60  298   79.00000   69.16186  12.5%   135  320s
 650012 181585   76.36274   55  321   79.00000   69.24612  12.3%   135  326s
 655785 182475   74.26275   47  333   79.00000   69.30972  12.3%   136  330s
 663916 183419     cutoff   50        79.00000   69.40966  12.1%   136  335s
 668291 184271   77.09132   58  268   79.00000   69.45866  12.1%   136  343s
 674223 184421   73.74102   54  266   79.00000   69.51101  12.0%   137  345s
 680430 185033     cutoff   51        79.00000   69.59797  11.9%   137  350s
 689473 185891   76.76569   50  229   79.00000   69.70165  11.8%   138  356s
 695189 186414   78.16896   56  231   79.00000   69.77456  11.7%   138  360s
 701768 186798   77.13072   53  252   79.00000   69.84151  11.6%   139  365s
 709745 187379   76.81903   55  315   79.00000   69.93614  11.5%   139  371s
 715450 187658   76.79408   55  259   79.00000   70.00463  11.4%   140  375s
 720916 188014   78.62710   56  246   79.00000   70.06684  11.3%   140  380s
 727192 188318   77.86033   54  319   79.00000   70.13649  11.2%   140  385s
 735488 188746   76.48884   54  292   79.00000   70.24178  11.1%   141  391s
 740909 188842     cutoff   54        79.00000   70.30236  11.0%   141  395s
 747296 189041     cutoff   59        79.00000   70.38163  10.9%   142  400s
 753843 189435     cutoff   47        79.00000   70.44827  10.8%   142  405s
 760173 189732   75.69999   51  246   79.00000   70.51745  10.7%   142  410s
 764975 189842     cutoff   53        79.00000   70.57373  10.7%   143  415s
 771856 189968   78.26851   56  296   79.00000   70.64777  10.6%   143  420s
 778323 190283     cutoff   51        79.00000   70.71695  10.5%   144  425s
 784208 190276     cutoff   53        79.00000   70.79511  10.4%   144  430s
 789864 190377     cutoff   59        79.00000   70.86051  10.3%   144  435s
 796028 190450   73.63428   48  332   79.00000   70.93024  10.2%   145  440s
 802266 190580     cutoff   60        79.00000   70.99909  10.1%   145  445s
 808601 190628   78.71811   56  309   79.00000   71.06811  10.0%   145  450s
 814104 190710   75.57225   49  244   79.00000   71.13305  10.0%   146  455s
 820121 190664   78.23751   57  172   79.00000   71.20252  9.87%   146  460s
 826387 190693   75.59182   55  258   79.00000   71.27335  9.78%   146  465s
 832147 190632     cutoff   59        79.00000   71.34382  9.69%   147  470s
 837843 190583   75.12960   49  318   79.00000   71.40114  9.62%   147  475s
 843444 190441   77.76185   53  286   79.00000   71.46734  9.54%   147  480s
 849698 190217   76.44121   53  311   79.00000   71.53251  9.45%   148  485s
 855860 190030     cutoff   56        79.00000   71.60606  9.36%   148  491s
 861603 189703   78.04546   54  323   79.00000   71.67964  9.27%   149  496s
 863743 189656   77.34505   52  276   79.00000   71.70841  9.23%   149  500s
 867115 189202     cutoff   55        79.00000   71.71860  9.22%   149  507s
 870832 188954     cutoff   52        79.00000   71.78219  9.14%   149  511s
 875416 188725   76.61204   59  304   79.00000   71.84712  9.05%   149  515s
 882255 188289     cutoff   50        79.00000   71.92306  8.96%   150  521s
 885978 187971     cutoff   53        79.00000   71.97773  8.89%   150  525s
 892377 187434   77.34806   53  232   79.00000   72.05215  8.79%   151  531s
 896551 186967     cutoff   52        79.00000   72.10551  8.73%   151  535s
 902576 186380     cutoff   58        79.00000   72.17854  8.63%   151  541s
 906559 185958   75.10543   50  331   79.00000   72.22981  8.57%   152  545s
 912654 185260     cutoff   55        79.00000   72.29805  8.48%   152  551s
 916657 184754   75.16154   52  306   79.00000   72.35017  8.42%   152  555s
 922559 184038   76.72543   55  307   79.00000   72.42884  8.32%   153  561s
 926474 183419     cutoff   57        79.00000   72.48002  8.25%   153  565s
 930367 182838     cutoff   57        79.00000   72.53012  8.19%   153  570s
 936185 181876     cutoff   55        79.00000   72.60841  8.09%   154  576s
 940202 181106     cutoff   54        79.00000   72.66194  8.02%   154  580s
 944069 180327     cutoff   53        79.00000   72.71562  7.95%   154  585s
 949793 179155     cutoff   53        79.00000   72.79233  7.86%   155  591s
 953862 178695   76.65498   53  297   79.00000   72.85285  7.78%   155  596s
 954949 178067   77.06045   55  296   79.00000   72.85835  7.77%   155  601s
 958762 177098     cutoff   56        79.00000   72.92547  7.69%   155  607s
 963211 176155   75.91747   54  222   79.00000   72.98963  7.61%   156  611s
 967750 175195   75.53202   54  295   79.00000   73.05295  7.53%   156  616s
 971764 174214   77.51928   55  293   79.00000   73.10802  7.46%   156  621s
 975476 173171     cutoff   46        79.00000   73.16601  7.38%   157  626s
 979585 172122     cutoff   53        79.00000   73.22647  7.31%   157  631s
 983932 171084     cutoff   55        79.00000   73.28841  7.23%   157  636s
 988044 169928   76.38637   52  309   79.00000   73.34339  7.16%   158  641s
 992077 168794     cutoff   58        79.00000   73.40607  7.08%   158  645s
 996087 167579     cutoff   50        79.00000   73.46644  7.00%   158  650s
 1000183 166391     cutoff   57        79.00000   73.52560  6.93%   158  655s
 1003988 165192     cutoff   55        79.00000   73.58428  6.86%   159  660s
 1007806 163978     cutoff   56        79.00000   73.63914  6.79%   159  665s
 1011983 162669     cutoff   49        79.00000   73.70318  6.70%   159  670s
 1016067 161276     cutoff   56        79.00000   73.76388  6.63%   160  675s
 1020218 159860     cutoff   49        79.00000   73.82670  6.55%   160  680s
 1024010 158384   78.54472   59  294   79.00000   73.89078  6.47%   160  685s
 1027891 156818   77.59063   51  325   79.00000   73.95455  6.39%   160  690s
 1031764 155201     cutoff   51        79.00000   74.01923  6.30%   161  695s
 1035461 153602     cutoff   53        79.00000   74.08491  6.22%   161  700s
 1039595 151873     cutoff   55        79.00000   74.15446  6.13%   161  706s
 1043572 150118     cutoff   59        79.00000   74.22089  6.05%   162  711s
 1047752 148310     cutoff   49        79.00000   74.28596  5.97%   162  716s
 1051450 146488     cutoff   53        79.00000   74.34830  5.89%   162  721s
 1055596 144489   77.47524   49  250   79.00000   74.42033  5.80%   162  727s
 1057790 143531   78.05909   59  218   79.00000   74.45494  5.75%   163  730s
 1061612 141445     cutoff   56        79.00000   74.52575  5.66%   163  735s
 1065698 139284     cutoff   58        79.00000   74.59406  5.58%   163  740s
 1069474 137211   78.17645   56  272   79.00000   74.65744  5.50%   163  746s
 1073694 134898     cutoff   63        79.00000   74.73676  5.40%   164  751s
 1077735 132694     cutoff   57        79.00000   74.80266  5.31%   164  756s
 1082051 130442     cutoff   51        79.00000   74.87327  5.22%   164  762s
 1085801 128018   77.85555   57  243   79.00000   74.94215  5.14%   164  767s
 1087770 126797     cutoff   55        79.00000   74.97748  5.09%   165  770s
 1091850 124255     cutoff   59        79.00000   75.05209  5.00%   165  775s
 1095500 121767     cutoff   58        79.00000   75.11452  4.92%   165  780s
 1099849 119126     cutoff   57        79.00000   75.19118  4.82%   165  786s
 1103975 116335     cutoff   57        79.00000   75.27128  4.72%   165  791s
 1108216 113570   78.46689   56  239   79.00000   75.34293  4.63%   166  797s
 1112297 110679     cutoff   56        79.00000   75.42495  4.53%   166  802s
 1114150 109240     cutoff   53        79.00000   75.46569  4.47%   166  805s
 1118375 106251     cutoff   51        79.00000   75.54383  4.37%   166  810s
 1122585 103228 infeasible   49        79.00000   75.62266  4.28%   166  815s
 1126312 100145     cutoff   56        79.00000   75.69730  4.18%   166  821s
 1130685 96940     cutoff   57        79.00000   75.77584  4.08%   167  826s
 1134860 93700   77.21750   52  293   79.00000   75.85585  3.98%   167  831s
 1138886 90396   78.03710   56  287   79.00000   75.93502  3.88%   167  836s
 1143051 87107   78.25826   54  308   79.00000   76.01811  3.77%   167  841s
 1147251 83737     cutoff   54        79.00000   76.09967  3.67%   167  847s
 1151045 80115     cutoff   60        79.00000   76.17459  3.58%   167  852s
 1155476 76340     cutoff   58        79.00000   76.26663  3.46%   167  857s
 1159566 72437     cutoff   64        79.00000   76.35425  3.35%   167  862s
 1163908 68591     cutoff   51        79.00000   76.43953  3.24%   167  867s
 1168267 65751     cutoff   53        79.00000   76.53422  3.12%   167  872s
 1169160 64272     cutoff   54        79.00000   76.54751  3.10%   167  875s
 1172705 60388     cutoff   56        79.00000   76.56006  3.09%   167  880s
 1177291 55584     cutoff   53        79.00000   76.69303  2.92%   167  885s
 1182351 50917     cutoff   53        79.00000   76.83631  2.74%   167  890s
 1189468 43873     cutoff   50        79.00000   76.99966  2.53%   167  896s
 1194174 39343     cutoff   63        79.00000   77.10013  2.40%   167  900s
 1201532 31995     cutoff   59        79.00000   77.28191  2.17%   167  906s
 1208850 24584     cutoff   58        79.00000   77.46146  1.95%   166  911s
 1216665 16698     cutoff   62        79.00000   77.72504  1.61%   166  915s
 1228503  3383     cutoff   66        79.00000   78.17932  1.04%   165  920s

Cutting planes:
  Learned: 2
  Gomory: 97
  Cover: 1065
  Dual implied bound: 20
  MIR: 412
  Mixing: 2
  StrongCG: 10
  Flow cover: 468
  GUB cover: 48
  Inf proof: 688
  Zero half: 3
  RLT: 1
  Relax-and-lift: 21

Explored 1236046 nodes (202798070 simplex iterations) in 920.81 seconds (3675.73 work units)
Thread count was 20 (of 20 available processors)

Solution count 4: 79 118 358 360 

Optimal solution found (tolerance 1.00e-04)
Best objective 7.900000000000e+01, best bound 7.900000000000e+01, gap 0.0000%

H1 Solution:
  Objective: 79.000000
  Selected features: 6/6
  L1 norm: 0.000000
  Positive accuracy lb: 0.0000
  Negative accuracy lb: 1.0000

============================================================
Training H2: Class {Class 2, 3} (+1) vs Class 1 (-1)
============================================================
H2 Training samples: 120
  Positive (+1): 81 samples
  Negative (-1): 39 samples

Set parameter TimeLimit to value 1800
Set parameter MIPGap to value 0.0001
Set parameter OutputFlag to value 1
Set parameter Threads to value 0
Gurobi Optimizer version 13.0.1 build v13.0.1rc0 (linux64 - "Ubuntu 24.04.3 LTS")

CPU model: AMD Ryzen 9 9900X 12-Core Processor, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 20 physical cores, 20 logical processors, using up to 20 threads

Non-default parameters:
TimeLimit  1800

Optimize a model with 854 rows, 501 columns and 3278 nonzeros (Min)
Model fingerprint: 0xdea831e5
Model has 374 linear objective coefficients
Variable types: 135 continuous, 366 integer (366 binary)
Coefficient statistics:
  Matrix range     [1e-07, 1e+03]
  Objective range  [1e+00, 1e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 2e+02]

Found heuristic solution: objective 77.0000000
Presolve removed 205 rows and 121 columns
Presolve time: 0.00s
Presolved: 649 rows, 380 columns, 2378 nonzeros
Variable types: 12 continuous, 368 integer (276 binary)

Root relaxation: objective -1.803000e+00, 275 iterations, 0.00 seconds (0.01 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0   -1.80300    0   90   77.00000   -1.80300   102%     -    0s
H    0     0                      10.0000000   -1.80300   118%     -    0s
H    0     0                       0.0000000   -1.80300      -     -    0s
H    0     0                      -0.0000000   -1.80300      -     -    0s

Cutting planes:
  Gomory: 9
  Implied bound: 30

Explored 1 nodes (275 simplex iterations) in 0.01 seconds (0.02 work units)
Thread count was 20 (of 20 available processors)

Solution count 3: -1e-10 10 77 

Optimal solution found (tolerance 1.00e-04)
Best objective -1.000000082740e-10, best bound -1.000000082740e-10, gap 0.0000%

H2 Solution:
  Objective: -0.000000
  Selected features: 6/6
  L1 norm: 2.000000
  Positive accuracy lb: 1.0000
  Negative accuracy lb: 1.0000

============================================================
Training Complete!
============================================================

Training completed in: 0:15:20.838536

================================================================================
Model Summary
================================================================================
Status: fitted
Strategy: inverted
Features: 6

Class Roles:
  Majority: Class 3
  Medium:   Class 2
  Minority: Class 1

Classifier 1 (H1): Class 2 vs {Class 3, 1}
  Objective: 79.000000
  Selected Features: 6/6
  L1 Norm: 0.000000

Classifier 2 (H2): Class {Class 2, 3} vs Class 1
  Objective: -0.000000
  Selected Features: 6/6
  L1 Norm: 2.000000

================================================================================
Training Set Evaluation
================================================================================


============================================================
Evaluation Results
============================================================

Total Accuracy: 0.6667

Per-Class Accuracy:
  Class 1: 1.0000
  Class 2: 0.0000
  Class 3: 1.0000

Class Distribution:
  Class 1: 39 samples
  Class 2: 40 samples
  Class 3: 41 samples

Confusion Matrix:
     Pred 1  Pred 2  Pred 3
True 1:    39       0       0
True 2:     0       0      40
True 3:     0       0      41
============================================================

================================================================================
Test Set Evaluation
================================================================================

Loading test data (skiprows=4)...
Loaded multi-class data:
  Class 1: 10 samples
  Class 2: 10 samples
  Class 3: 11 samples
  Features: 5
Test data loaded:
  Class 1: 10 samples
  Class 2: 10 samples
  Class 3: 11 samples

Warning: Could not load or evaluate test data: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 6 is different from 5)
Skipping test set evaluation.

================================================================================
Test Complete!
================================================================================
Training Duration: 0:15:20.838536
Training Accuracy: 0.6667
Log saved to: /home/bs10081/Developer/hcesvm/results/inverted_TAE_20260131_133312.log
================================================================================

================================================================================
Hierarchical CE-SVM - TEST2 Strategy
Dataset: TAE
Timestamp: 2026-02-10 19:34:34
================================================================================

Training data:
  Class 1: 39 samples
  Class 2: 40 samples
  Class 3: 41 samples
  Features: 5

================================================================================
Model Parameters
================================================================================
  C_hyper: 1.0
  epsilon: 0.0001
  M: 1000.0
  time_limit: 1800
  mip_gap: 0.0001
  threads: 0
  verbose: True
  enable_selection: True
  feat_upper_bound: 1000
  feat_lower_bound: 1e-07

================================================================================
Training Hierarchical Classifier - TEST2 Strategy
================================================================================

Test2 Strategy:
  H1: {minority, medium} vs majority
  H2: minority vs medium
  Objective function modified based on class roles

============================================================
Training Hierarchical CE-SVM (Strategy: test2)
============================================================
Class 1: 39 samples
Class 2: 40 samples
Class 3: 41 samples
Features: 5

Dynamic Class Roles:
  Majority:  Class 3 (41 samples)
  Medium:    Class 2 (40 samples)
  Minority:  Class 1 (39 samples)

============================================================
Training H1: Class {2, 3} (+1) vs Class 1 (-1)
============================================================
H1 Training samples: 120
  Positive (+1): 81 samples
  Negative (-1): 39 samples
  Accuracy mode: both

Set parameter TimeLimit to value 1800
Set parameter MIPGap to value 0.0001
Set parameter OutputFlag to value 1
Set parameter Threads to value 0
Gurobi Optimizer version 13.0.1 build v13.0.1rc0 (linux64 - "Ubuntu 24.04.3 LTS")

CPU model: AMD Ryzen 9 9900X 12-Core Processor, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 20 physical cores, 20 logical processors, using up to 20 threads

Non-default parameters:
TimeLimit  1800

Optimize a model with 852 rows, 498 columns and 3032 nonzeros (Min)
Model fingerprint: 0xb84f3c9c
Model has 372 linear objective coefficients
Variable types: 133 continuous, 365 integer (365 binary)
Coefficient statistics:
  Matrix range     [1e-07, 1e+03]
  Objective range  [1e+00, 1e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 2e+02]

Found heuristic solution: objective 272.1950079
Presolve removed 217 rows and 127 columns
Presolve time: 0.00s
Presolved: 635 rows, 371 columns, 2270 nonzeros
Variable types: 11 continuous, 360 integer (270 binary)

Root relaxation: objective -1.803000e+00, 257 iterations, 0.00 seconds (0.00 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0   -1.80300    0   90  272.19501   -1.80300   101%     -    0s
H    0     0                      77.0000000   -1.80300   102%     -    0s
     0     0   -1.14328    0  199   77.00000   -1.14328   101%     -    0s
     0     0    2.23961    0  243   77.00000    2.23961  97.1%     -    0s
     0     0    4.87783    0  230   77.00000    4.87783  93.7%     -    0s
     0     0    4.92045    0  242   77.00000    4.92045  93.6%     -    0s
     0     0    5.03050    0  242   77.00000    5.03050  93.5%     -    0s
     0     0    9.32283    0  221   77.00000    9.32283  87.9%     -    0s
     0     0   10.81439    0  233   77.00000   10.81439  86.0%     -    0s
     0     0   10.90231    0  229   77.00000   10.90231  85.8%     -    0s
     0     0   10.90331    0  229   77.00000   10.90331  85.8%     -    0s
     0     0   13.27895    0  208   77.00000   13.27895  82.8%     -    0s
     0     0   13.37799    0  202   77.00000   13.37799  82.6%     -    0s
     0     0   13.38630    0  212   77.00000   13.38630  82.6%     -    0s
     0     0   13.39060    0  215   77.00000   13.39060  82.6%     -    0s
     0     0   13.57616    0  245   77.00000   13.57616  82.4%     -    0s
     0     0   13.59625    0  245   77.00000   13.59625  82.3%     -    0s
     0     0   13.59974    0  250   77.00000   13.59974  82.3%     -    0s
     0     0   14.55049    0  169   77.00000   14.55049  81.1%     -    0s
     0     0   14.63708    0  232   77.00000   14.63708  81.0%     -    0s
     0     0   14.64960    0  229   77.00000   14.64960  81.0%     -    0s
     0     0   14.64964    0  229   77.00000   14.64964  81.0%     -    0s
     0     0   14.70075    0  244   77.00000   14.70075  80.9%     -    0s
     0     0   14.71586    0  238   77.00000   14.71586  80.9%     -    0s
     0     0   14.72650    0  237   77.00000   14.72650  80.9%     -    0s
     0     0   14.73371    0  209   77.00000   14.73371  80.9%     -    0s
     0     0   14.79603    0  169   77.00000   14.79603  80.8%     -    0s
     0     0   14.79794    0  166   77.00000   14.79794  80.8%     -    0s
     0     0   14.90480    0  230   77.00000   14.90480  80.6%     -    0s
     0     0   15.12231    0  232   77.00000   15.12231  80.4%     -    0s
     0     0   15.20380    0  232   77.00000   15.20380  80.3%     -    0s
     0     0   15.21274    0  246   77.00000   15.21274  80.2%     -    0s
     0     0   15.21454    0  241   77.00000   15.21454  80.2%     -    0s
     0     0   16.04029    0  187   77.00000   16.04029  79.2%     -    0s
     0     0   16.17796    0  236   77.00000   16.17796  79.0%     -    0s
     0     0   16.18706    0  245   77.00000   16.18706  79.0%     -    0s
     0     0   16.19426    0  245   77.00000   16.19426  79.0%     -    0s
     0     0   16.19532    0  245   77.00000   16.19532  79.0%     -    0s
     0     0   17.48979    0  225   77.00000   17.48979  77.3%     -    0s
     0     0   17.53096    0  230   77.00000   17.53096  77.2%     -    0s
     0     0   17.53218    0  233   77.00000   17.53218  77.2%     -    0s
     0     0   17.61218    0  231   77.00000   17.61218  77.1%     -    0s
     0     0   17.79036    0  222   77.00000   17.79036  76.9%     -    0s
     0     0   17.88372    0  218   77.00000   17.88372  76.8%     -    0s
     0     0   18.00860    0  232   77.00000   18.00860  76.6%     -    0s
     0     0   18.12792    0  225   77.00000   18.12792  76.5%     -    0s
     0     0   18.13204    0  219   77.00000   18.13204  76.5%     -    0s
     0     0   18.96344    0  209   77.00000   18.96344  75.4%     -    0s
     0     0   19.06890    0  215   77.00000   19.06890  75.2%     -    0s
     0     0   19.16384    0  216   77.00000   19.16384  75.1%     -    0s
     0     0   19.18922    0  222   77.00000   19.18922  75.1%     -    0s
     0     0   19.20059    0  227   77.00000   19.20059  75.1%     -    0s
     0     0   19.20066    0  227   77.00000   19.20066  75.1%     -    0s
     0     0   19.81750    0  134   77.00000   19.81750  74.3%     -    0s
     0     0   20.42003    0  229   77.00000   20.42003  73.5%     -    0s
     0     0   20.44676    0  227   77.00000   20.44676  73.4%     -    0s
     0     0   20.45112    0  223   77.00000   20.45112  73.4%     -    0s
     0     0   20.56244    0  217   77.00000   20.56244  73.3%     -    0s
     0     0   20.64420    0  229   77.00000   20.64420  73.2%     -    0s
     0     0   20.66396    0  238   77.00000   20.66396  73.2%     -    0s
     0     0   20.66924    0  232   77.00000   20.66924  73.2%     -    0s
     0     0   20.75270    0  222   77.00000   20.75270  73.0%     -    0s
     0     0   20.80004    0  223   77.00000   20.80004  73.0%     -    0s
     0     0   20.80012    0  225   77.00000   20.80012  73.0%     -    0s
     0     0   21.09162    0  234   77.00000   21.09162  72.6%     -    0s
     0     0   21.36025    0  129   77.00000   21.36025  72.3%     -    0s
     0     0   21.40021    0  129   77.00000   21.40021  72.2%     -    0s
     0     0   21.49594    0  234   77.00000   21.49594  72.1%     -    0s
     0     0   21.52740    0  206   77.00000   21.52740  72.0%     -    0s
     0     0   21.54548    0  220   77.00000   21.54548  72.0%     -    0s
     0     0   21.55149    0  226   77.00000   21.55149  72.0%     -    0s
     0     0   21.56886    0  225   77.00000   21.56886  72.0%     -    0s
     0     0   21.57256    0  230   77.00000   21.57256  72.0%     -    0s
     0     0   21.57256    0  230   77.00000   21.57256  72.0%     -    0s
     0     0   21.57256    0  230   77.00000   21.57256  72.0%     -    0s
     0     2   21.57300    0  230   77.00000   21.57300  72.0%     -    0s
 25011 12997   64.47258   37  223   77.00000   55.99673  27.3%  41.3    5s
 59294 16789   69.30777   37  308   77.00000   63.92313  17.0%  58.6   10s
 82300 23139   69.56168   42  188   77.00000   66.03762  14.2%  66.2   15s
 107439 27034   73.66638   40  297   77.00000   67.92917  11.8%  74.8   20s
 130271 28928   73.92259   40  242   77.00000   69.41592  9.85%  81.3   25s
 149528 28032     cutoff   45        77.00000   70.67164  8.22%  87.6   30s
 164137 25848   73.51291   44  239   77.00000   71.54152  7.09%  92.3   35s
 180104 21219     cutoff   42        77.00000   72.81125  5.44%  97.3   40s
 195993 10482   76.14411   57  132   77.00000   74.37788  3.41%   102   45s

Cutting planes:
  Gomory: 30
  Cover: 213
  Clique: 2
  MIR: 151
  Flow cover: 255
  GUB cover: 16
  Inf proof: 377
  Zero half: 11
  RLT: 2
  Relax-and-lift: 5

Explored 209153 nodes (20976868 simplex iterations) in 47.05 seconds (141.12 work units)
Thread count was 20 (of 20 available processors)

Solution count 2: 77 272.195 

Optimal solution found (tolerance 1.00e-04)
Best objective 7.700000000000e+01, best bound 7.700000000000e+01, gap 0.0000%

H1 Solution:
  Weights (w): [0. 0. 0. 0. 0.]
  Intercept (b): 1.0
  Objective: 77.000000
  Selected features: 5/5
  L1 norm: 0.000000
  Positive accuracy lb: 1.0000
  Negative accuracy lb: 0.0000

============================================================
Training H2: Class 3 (+1) vs Class {1, 2} (-1)
============================================================
H2 Training samples: 120
  Positive (+1): 41 samples
  Negative (-1): 79 samples
  Accuracy mode: both

Set parameter TimeLimit to value 1800
Set parameter MIPGap to value 0.0001
Set parameter OutputFlag to value 1
Set parameter Threads to value 0
Gurobi Optimizer version 13.0.1 build v13.0.1rc0 (linux64 - "Ubuntu 24.04.3 LTS")

CPU model: AMD Ryzen 9 9900X 12-Core Processor, instruction set [SSE2|AVX|AVX2|AVX512]
Thread count: 20 physical cores, 20 logical processors, using up to 20 threads

Non-default parameters:
TimeLimit  1800

Optimize a model with 852 rows, 498 columns and 3032 nonzeros (Min)
Model fingerprint: 0x7ba99dba
Model has 372 linear objective coefficients
Variable types: 133 continuous, 365 integer (365 binary)
Coefficient statistics:
  Matrix range     [1e-07, 1e+03]
  Objective range  [1e+00, 1e+00]
  Bounds range     [1e+00, 1e+00]
  RHS range        [1e+00, 2e+02]

Presolve removed 210 rows and 123 columns
Presolve time: 0.00s
Presolved: 642 rows, 375 columns, 2290 nonzeros
Variable types: 11 continuous, 364 integer (273 binary)
Found heuristic solution: objective 360.0000000
Found heuristic solution: objective 359.0000000
Found heuristic solution: objective 118.0000000

Root relaxation: objective -1.793000e+00, 280 iterations, 0.00 seconds (0.01 work units)

    Nodes    |    Current Node    |     Objective Bounds      |     Work
 Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time

     0     0   -1.79300    0   93  118.00000   -1.79300   102%     -    0s
H    0     0                      81.0000000   -1.79300   102%     -    0s
     0     0    0.36103    0  244   81.00000    0.36103   100%     -    0s
     0     0    0.95197    0  237   81.00000    0.95197  98.8%     -    0s
     0     0    6.56433    0  217   81.00000    6.56433  91.9%     -    0s
     0     0    7.43919    0  196   81.00000    7.43919  90.8%     -    0s
     0     0    7.44322    0  199   81.00000    7.44322  90.8%     -    0s
     0     0    7.44422    0  204   81.00000    7.44422  90.8%     -    0s
     0     0    9.68965    0  207   81.00000    9.68965  88.0%     -    0s
     0     0   10.05852    0  197   81.00000   10.05852  87.6%     -    0s
     0     0   10.10371    0  199   81.00000   10.10371  87.5%     -    0s
     0     0   10.10471    0  199   81.00000   10.10471  87.5%     -    0s
H    0     0                      79.4963065   10.10471  87.3%     -    0s
H    0     0                      77.8329731   10.10471  87.0%     -    0s
H    0     0                      76.8329731   10.10471  86.8%     -    0s
     0     0   12.44790    0  212   76.83297   12.44790  83.8%     -    0s
     0     0   12.55488    0  208   76.83297   12.55488  83.7%     -    0s
     0     0   12.55982    0  216   76.83297   12.55982  83.7%     -    0s
     0     0   13.61882    0  219   76.83297   13.61882  82.3%     -    0s
     0     0   13.62886    0  222   76.83297   13.62886  82.3%     -    0s
     0     0   13.63485    0  216   76.83297   13.63485  82.3%     -    0s
     0     0   14.32816    0  198   76.83297   14.32816  81.4%     -    0s
     0     0   14.35263    0  203   76.83297   14.35263  81.3%     -    0s
     0     0   14.74627    0  185   76.83297   14.74627  80.8%     -    0s
     0     0   14.84226    0  203   76.83297   14.84226  80.7%     -    0s
     0     0   14.85325    0  197   76.83297   14.85325  80.7%     -    0s
     0     0   14.85423    0  197   76.83297   14.85423  80.7%     -    0s
     0     0   15.02115    0  194   76.83297   15.02115  80.4%     -    0s
     0     0   15.18131    0  203   76.83297   15.18131  80.2%     -    0s
     0     0   15.19921    0  196   76.83297   15.19921  80.2%     -    0s
     0     0   15.21180    0  198   76.83297   15.21180  80.2%     -    0s
     0     0   15.21381    0  195   76.83297   15.21381  80.2%     -    0s
     0     0   16.27458    0  205   76.83297   16.27458  78.8%     -    0s
     0     0   16.73827    0  180   76.83297   16.73827  78.2%     -    0s
     0     0   16.81648    0  194   76.83297   16.81648  78.1%     -    0s
     0     0   16.90784    0  196   76.83297   16.90784  78.0%     -    0s
     0     0   16.91644    0  191   76.83297   16.91644  78.0%     -    0s
     0     0   16.93558    0  194   76.83297   16.93558  78.0%     -    0s
     0     0   16.93592    0  194   76.83297   16.93592  78.0%     -    0s
     0     0   16.94666    0  193   76.83297   16.94666  77.9%     -    0s
     0     0   16.94796    0  194   76.83297   16.94796  77.9%     -    0s
     0     0   17.60667    0  249   76.83297   17.60667  77.1%     -    0s
     0     0   17.88105    0  206   76.83297   17.88105  76.7%     -    0s
     0     0   17.88813    0  212   76.83297   17.88813  76.7%     -    0s
     0     0   17.99660    0  208   76.83297   17.99660  76.6%     -    0s
     0     0   18.35354    0  198   76.83297   18.35354  76.1%     -    0s
     0     0   18.53658    0  196   76.83297   18.53658  75.9%     -    0s
     0     0   18.55611    0  207   76.83297   18.55611  75.8%     -    0s
     0     0   18.56271    0  197   76.83297   18.56271  75.8%     -    0s
     0     0   19.55015    0  247   76.83297   19.55015  74.6%     -    0s
     0     0   19.73961    0  252   76.83297   19.73961  74.3%     -    0s
     0     0   19.75147    0  245   76.83297   19.75147  74.3%     -    0s
     0     0   19.75650    0  245   76.83297   19.75650  74.3%     -    0s
     0     0   19.85595    0  234   76.83297   19.85595  74.2%     -    0s
     0     0   20.33941    0  159   76.83297   20.33941  73.5%     -    0s
     0     0   20.34553    0  165   76.83297   20.34553  73.5%     -    0s
     0     0   20.38761    0  166   76.83297   20.38761  73.5%     -    0s
     0     0   20.42428    0  165   76.83297   20.42428  73.4%     -    0s
     0     0   20.42447    0  167   76.83297   20.42447  73.4%     -    0s
     0     0   20.42984    0  237   76.83297   20.42984  73.4%     -    0s
     0     0   20.43541    0  237   76.83297   20.43541  73.4%     -    0s
     0     0   20.43541    0  237   76.83297   20.43541  73.4%     -    0s
     0     2   20.43541    0  237   76.83297   20.43541  73.4%     -    0s
 27124 11131   74.10516   43  206   76.83297   55.33375  28.0%  41.1    5s
H41227 12332                      75.6894103   60.09985  20.6%  47.6    6s
 65387 16843   68.34129   37  241   75.68941   63.74331  15.8%  55.2   10s
 85995 20568   69.32717   43  230   75.68941   65.98129  12.8%  63.4   15s
 110375 20560   75.18499   47  157   75.68941   68.63264  9.32%  72.9   20s
 128456 16172     cutoff   51        75.68941   70.83710  6.41%  80.6   25s
 143611  4715     cutoff   47        75.68941   73.21618  3.27%  85.7   30s

Cutting planes:
  Gomory: 47
  Cover: 148
  Dual implied bound: 4
  MIR: 142
  StrongCG: 3
  Flow cover: 229
  GUB cover: 10
  Inf proof: 346
  Zero half: 8
  Relax-and-lift: 5

Explored 150708 nodes (12704598 simplex iterations) in 30.78 seconds (86.27 work units)
Thread count was 20 (of 20 available processors)

Solution count 7: 75.6894 76.833 77.833 ... 360

Optimal solution found (tolerance 1.00e-04)
Best objective 7.568941031182e+01, best bound 7.568941031182e+01, gap 0.0000%

H2 Solution:
  Weights (w): [-2.  0.  0. -2.  0.]
  Intercept (b): 7.0
  Objective: 75.689410
  Selected features: 5/5
  L1 norm: 4.000000
  Positive accuracy lb: 0.4878
  Negative accuracy lb: 0.8228

============================================================
Training Complete!
============================================================

Training completed in: 0:01:17.850654

================================================================================
Model Summary
================================================================================
Status: fitted
Strategy: test2
Features: 5

Class Roles:
  Majority: Class 3
  Medium:   Class 2
  Minority: Class 1

Classifier 1 (H1): Class {2, 3} vs Class 1
  Objective: 77.000000
  Selected Features: 5/5
  L1 Norm: 0.000000

Classifier 2 (H2): Class 3 vs Class {1, 2}
  Objective: 75.689410
  Selected Features: 5/5
  L1 Norm: 4.000000

================================================================================
H1 Complete Weights and Bias
================================================================================
Description: Class {2, 3} vs Class 1

Intercept (b): 1.0000000000

Weight vector (w) - 5 features:
  w[0] = 0.0000000000
  w[1] = 0.0000000000
  w[2] = 0.0000000000
  w[3] = 0.0000000000
  w[4] = 0.0000000000

L1 Norm: 0.0000000000

================================================================================
H2 Complete Weights and Bias
================================================================================
Description: Class 3 vs Class {1, 2}

Intercept (b): 7.0000000000

Weight vector (w) - 5 features:
  w[0] = -2.0000000000
  w[1] = 0.0000000000
  w[2] = 0.0000000000
  w[3] = -2.0000000000
  w[4] = 0.0000000000

L1 Norm: 4.0000000000

================================================================================
Training Set Evaluation
================================================================================


============================================================
Evaluation Results
============================================================

Total Accuracy: 0.4250

Per-Class Accuracy:
  Class 1: 0.0000
  Class 2: 0.7750
  Class 3: 0.4878

Class Distribution:
  Class 1: 39 samples
  Class 2: 40 samples
  Class 3: 41 samples

Confusion Matrix:
     Pred 1  Pred 2  Pred 3
True 1:     0      34       5
True 2:     0      31       9
True 3:     0      21      20
============================================================

================================================================================
Test Set Evaluation
================================================================================


============================================================
Evaluation Results
============================================================

Total Accuracy: 0.4839

Per-Class Accuracy:
  Class 1: 0.0000
  Class 2: 0.8000
  Class 3: 0.6364

Class Distribution:
  Class 1: 10 samples
  Class 2: 10 samples
  Class 3: 11 samples

Confusion Matrix:
     Pred 1  Pred 2  Pred 3
True 1:     0      10       0
True 2:     0       8       2
True 3:     0       4       7
============================================================

================================================================================
Test Complete!
================================================================================
Training Duration: 0:01:17.850654
Training Accuracy: 0.4250
Test Accuracy: 0.4839
Log saved to: /home/bs10081/Developer/hcesvm/results/test2_TAE_20260210_193434.log
================================================================================
